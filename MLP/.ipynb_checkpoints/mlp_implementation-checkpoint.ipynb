{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.contrib import layers\n",
    "import sklearn.metrics as Metrics\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import os\n",
    "\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.gpu_id = 1\n",
    "        ###################### Data ##########################\n",
    "        # Number of diagnosis codes\n",
    "        self.num_dcode = None\n",
    "        #################### Embeddings #########################        \n",
    "        # Vector size for each word embeddings from GloVe\n",
    "        self.emb_size = 300\n",
    "        ###################### Model ######################### \n",
    "        # Training Batch Size\n",
    "        self.batch_size = 40\n",
    "        # Epoch\n",
    "        self.epoch = 251\n",
    "        # Learning rate\n",
    "        self.lr_rate = 1e-3\n",
    "        # keep_prob, dropout_rate = 1 - keep_prob, here is the keep_prob rate\n",
    "        self.keep_prob = 0.8\n",
    "        # Hidden Layer\n",
    "        self.H_dis = 300\n",
    "        # Optimizer\n",
    "        self.optimizer = 'Adam'\n",
    "        # Validation Frequency\n",
    "        self.valid_freq = 100\n",
    "        # Early Stopping\n",
    "        self.early_stop = False\n",
    "        # Patience\n",
    "        self.patience = None\n",
    "        opt.cur_num = 0\n",
    "        opt.num_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\" shuffle the dataset and get minibatches\n",
    "    \n",
    "    Args:\n",
    "        n: size of the dataset\n",
    "        minibatch_size: size of a desired minibatch size\n",
    "        shuffle: an option to shuffle the dataset before getting minibatches. Default to False.\n",
    "    \n",
    "    Return:\n",
    "        zipped iterable that contains the minibatch size and minibatches\n",
    "    \"\"\"\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "        \n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_2layer(H, opt, dropout, is_training, prefix='', num_outputs=1, is_reuse=None):\n",
    "    \"\"\" Apply two fully connected layers with each layer batch-normalized, last layer is linear\n",
    "    \n",
    "    Args:\n",
    "        H: encoder input\n",
    "        opt: option class\n",
    "        dropout: keep rate for dropout layers\n",
    "        is_training: training flag for batch normalization\n",
    "        \n",
    "    Return:\n",
    "        logits: logits for one batch. Further calculate the probability by using sigmoid/softmax.         \n",
    "    \"\"\"\n",
    "    # biasInit = tf.constant_initializer(0.001, dtype=tf.float32)\n",
    "    H_dis_ = tf.layers.dense(tf.nn.dropout(H, keep_prob=dropout), units=opt.H_dis,\n",
    "                                   activation=None, name=prefix + 'dis_1', use_bias=False,\n",
    "                                   reuse=is_reuse)\n",
    "    H_dis_norm = tf.layers.batch_normalization(H_dis_, training=is_training)\n",
    "    H_dis = tf.nn.relu(H_dis_norm, 'relu')\n",
    "    \n",
    "    logits = tf.layers.dense(tf.nn.dropout(H_dis, keep_prob=dropout), units=num_outputs,\n",
    "                           use_bias=False, name=prefix + 'dis_2', reuse=is_reuse)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_classifier(x, y, keep_prob, opt, is_training):\n",
    "    \"\"\"MLP model\"\"\"\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    logits = discriminator_2layer(x, opt, keep_prob, is_training)\n",
    "    prob = tf.nn.sigmoid(logits)    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_step = tf.train.AdamOptimizer(opt.lr_rate).minimize(loss)\n",
    "    return prob, loss, train_step, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_data, train_label, val_data, val_label, opt):\n",
    "    \"\"\"Implement MLP\"\"\"\n",
    "    n_train = len(train_data)\n",
    "    n_val = len(val_data)\n",
    "    tf.set_random_seed(123)\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x_ = tf.placeholder(tf.int32, shape=[None, opt.num_dcode])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    is_training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    prob_, loss_, train_op, saver_ = emb_classifier(x_, y_, keep_prob, opt, is_training)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_auc_list = []\n",
    "        val_auc_list = []\n",
    "\n",
    "        # Validation\n",
    "        uidx = 0\n",
    "        stop_uidx = 0\n",
    "        max_val_auc = 0\n",
    "        counter = 0\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        \n",
    "        for epoch in range(opt.epoch):\n",
    "            train_minibatch_idx = get_minibatches_idx(n_train, opt.batch_size, shuffle=True)\n",
    "            for _, train_index in train_minibatch_idx:\n",
    "                uidx += 1\n",
    "                # Exponential Learning rate decay\n",
    "\n",
    "                if opt.early_stop == False:\n",
    "                    # x\n",
    "                    x_batch = [train_data[t] for t in train_index]\n",
    "                    # labels\n",
    "                    x_labels = [train_label[t] for t in train_index]\n",
    "                    x_labels = np.array(x_labels)\n",
    "                    x_labels = x_labels.reshape([-1,1])\n",
    "\n",
    "                    _, loss = sess.run([train_op, loss_], feed_dict={x_: x_batch, y_: x_labels, \n",
    "                                                                     keep_prob: opt.keep_prob, is_training: True})\n",
    "\n",
    "                    if uidx % opt.valid_freq == 0:\n",
    "#                         print('Iteration: -------------%d------------' % uidx)\n",
    "                        train_prob_list = []\n",
    "                        train_true_list = []\n",
    "                        train_loss_list = []\n",
    "                        H_enc_train = []\n",
    "                        \n",
    "\n",
    "                        kf_train = get_minibatches_idx(opt.samples, opt.batch_size, shuffle = False)\n",
    "                        for _, train_index in kf_train:\n",
    "                            # x\n",
    "                            x_train_batch = [train_data[t] for t in train_index]\n",
    "                            # labels\n",
    "                            train_labels = [train_label[t] for t in train_index]\n",
    "                            train_labels = np.array(train_labels)\n",
    "                            train_labels = train_labels.reshape([-1, 1])\n",
    "                            \n",
    "                            train_prob, train_loss0 = sess.run([prob_,loss_], feed_dict={x_: x_train_batch,\n",
    "                                                                   y_: train_labels, keep_prob: 1.0})\n",
    "                            train_prob_list += train_prob.tolist()\n",
    "                            train_true_list += train_labels.tolist()\n",
    "                            train_loss_list.append(train_loss0)\n",
    "                            \n",
    "                        # Calculate train AUC score at iteration uidx\n",
    "                        train_prob_array = np.asarray(train_prob_list)\n",
    "                        train_true_array = np.asarray(train_true_list)\n",
    "                        train_loss_array = np.array(train_loss_list)\n",
    "                        train_loss.append(np.mean(train_loss_array))\n",
    "                        train_auc = Metrics.roc_auc_score(train_true_array, train_prob_array)\n",
    "                        train_auc_list.append(train_auc)\n",
    "\n",
    "                        # Validation\n",
    "                        val_prob_list = []\n",
    "                        val_true_list = []\n",
    "                        val_loss_list = []\n",
    "                        val_minibatch = get_minibatches_idx(n_val, opt.batch_size, shuffle=False)\n",
    "                        for _, val_id in val_minibatch:\n",
    "                            # x\n",
    "                            x_val_batch = [val_data[t] for t in val_id]\n",
    "                            # labels\n",
    "                            val_labels = [val_label[idx_y] for idx_y in val_id]\n",
    "                            val_labels = np.array(val_labels).reshape([-1, 1])\n",
    "\n",
    "                            val_prob, val_loss0 = sess.run([prob_, loss_], feed_dict = {x_:x_val_batch,\n",
    "                                                                    y_:val_labels, keep_prob:1.0})\n",
    "\n",
    "                            val_prob_list += val_prob.tolist()\n",
    "                            val_true_list += val_labels.tolist()\n",
    "                            val_loss_list.append(val_loss0)\n",
    "                            \n",
    "                        # Calculate validation accuracy and AUC\n",
    "                        val_prob_array = np.asarray(val_prob_list)\n",
    "                        val_true_array = np.asarray(val_true_list)\n",
    "                        val_loss_array = np.array(val_loss_list)\n",
    "                        val_loss.append(np.mean(val_loss_array))\n",
    "\n",
    "                        val_auc = Metrics.roc_auc_score(val_true_array, val_prob_array)\n",
    "                        val_auc_list.append(val_auc)\n",
    "\n",
    "                        if val_auc > max_val_auc:\n",
    "                            stop_uidx = uidx\n",
    "                            curr_patience = 0\n",
    "                            max_val_auc = val_auc\n",
    "                            saver_.save(sess, './save/mlp_dcode/'+str(opt.num_cur))\n",
    "\n",
    "                        elif curr_patience < opt.patience:\n",
    "                            curr_patience += 1\n",
    "#                             print(curr_patience)\n",
    "                        if curr_patience == opt.patience:\n",
    "                            opt.early_stop = False   \n",
    "                else:\n",
    "                    break\n",
    "            if opt.early_stop == True:\n",
    "                print(\"Early stopping at epoch {0}: \\t iteration: {1} \\t max val AUC: {2}\".format(epoch, stop_uidx, \n",
    "                                                                                                round(max_val_auc,4)))\n",
    "                break\n",
    "    return [train_auc_list, val_auc_list, train_loss, val_loss]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
