{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAM TCN MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import spatial\n",
    "import sklearn.metrics as Metrics\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        # GPU id\n",
    "        self.gpu_id = 0\n",
    "        ###################### Data ##########################\n",
    "        # Random state of spliting data\n",
    "        self.rs = None\n",
    "        self.num_input = None\n",
    "        self.num_class = None\n",
    "        self.timestep = None\n",
    "\n",
    "        ###################### Model #########################\n",
    "        # Training Batch Size\n",
    "        self.batch_size = 20\n",
    "        # Epoch\n",
    "        self.epoch = 251\n",
    "        # Learning rate\n",
    "        self.lr_rate = 0.001\n",
    "        # keep_prob, dropout_rate = 1 - keep_prob, here is the keep_prob rate\n",
    "        self.keep_prob = 0.8\n",
    "        # hidden units for notes\n",
    "        self.H_dis = 4\n",
    "        # Optimizer\n",
    "        self.optimizer = 'Adam'\n",
    "        # Validation Frequency\n",
    "        self.valid_freq = 100\n",
    "        # Early Stopping\n",
    "        self.early_stop = False\n",
    "        # Patience\n",
    "        self.patience = None\n",
    "        # Encoder\n",
    "        self.encoder = \"None\"\n",
    "        # Dilation rate\n",
    "        self.l = 1\n",
    "        # kernel size for tcn\n",
    "        self.k = 3\n",
    "        # number of filters\n",
    "        self.num_filters = 8\n",
    "        # save model path\n",
    "        self.save_path = './save/leam_att/att_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f659c5e1ce47>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1D(tf.layers.Conv1D):\n",
    "    def __init__(self, filters,\n",
    "               kernel_size,\n",
    "               strides=1,\n",
    "               dilation_rate=1,\n",
    "               activation=None,\n",
    "               use_bias=True,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=tf.zeros_initializer(),\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               trainable=True,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            inputs = tf.pad(inputs, tf.constant([[0, 0], [0, 0], [padding, 0]], dtype=tf.int32))\n",
    "        else:\n",
    "            inputs = tf.pad(inputs, tf.constant([(0, 0,), (padding, 0), (0, 0)]))\n",
    "        return super(CausalConv1D, self).call(inputs), inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jikai/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "(1, 9, 1)\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "(1, 7, 8)\n",
      "[-0.14872932  0.         -0.46489942 -0.14872932  0.         -0.46489942\n",
      " -0.14872932]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.expand_dims(\n",
    "        tf.expand_dims(tf.constant([1, 0, 0, 1, 0, 0, 1], dtype=tf.float32), axis=0),\n",
    "        axis=-1) # (batch_size, length, channel)\n",
    "    with tf.variable_scope(\"tcn\"):\n",
    "        conv = CausalConv1D(8, 2, dilation_rate=2, activation=None)\n",
    "    output = conv(x)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res, inputs = sess.run(output)\n",
    "    print(inputs.shape)\n",
    "    print(inputs[0, :, 0])\n",
    "    print(res.shape)    \n",
    "    print(res[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4, 10)\n",
      "[[ 0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  -0.0000000e+00 -0.0000000e+00]\n",
      " [-0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "   0.0000000e+00 -0.0000000e+00]\n",
      " [ 6.7740923e-01  1.9230318e+00 -1.7757033e+00  2.2743883e+00\n",
      "  -1.1445057e+00  4.4063632e-03  1.6304694e+00 -7.6573098e-01\n",
      "   2.0447132e-01 -1.7233037e+00]\n",
      " [-1.7778189e-01  3.2459620e-01  3.3309302e+00  1.6665771e+00\n",
      "   5.0001545e+00  2.5287453e-02  3.6113930e+00  3.0575074e-02\n",
      "   1.1064229e+00 -1.7591003e+00]]\n",
      "[[ 1.1488044  -0.20871945  0.13462071 -1.2171024  -1.6498863   2.488263\n",
      "  -0.992963    1.4148185  -0.73274577 -3.3760247 ]\n",
      " [ 1.3042604   2.2655525  -2.0222583  -2.3801963  -1.686778    0.6138112\n",
      "   2.658316   -1.1744169   2.2630975  -1.5531353 ]\n",
      " [-0.60847014  2.5457692   2.0453658   1.4912694   3.4782932   5.6132708\n",
      "   0.35136226  1.7942569   1.2821397   1.3024963 ]\n",
      " [ 4.0875554  -2.4857845  -2.110277    4.965964    0.6027449   0.62476087\n",
      "   0.39931136 -1.9685973  -2.7966135   3.6392758 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 4, 10)) # (batch_size, channel, length)\n",
    "    dropout = tf.layers.Dropout(0.5, noise_shape=[x.shape[0], x.shape[1], tf.constant(1)])\n",
    "    output = dropout(x, training=True)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, :])\n",
    "    print(res[1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(tf.layers.Layer):\n",
    "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.2, \n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )        \n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "        self.conv1 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "        self.conv2 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = 2\n",
    "        self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        if input_shape[channel_dim] != self.n_outputs:\n",
    "#             pass\n",
    "            # self.down_sample = tf.layers.Conv1D(\n",
    "            #     self.n_outputs, kernel_size=1, \n",
    "            #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
    "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)[0]\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)[0]\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        if self.down_sample is not None:\n",
    "            print('downsampling...')\n",
    "            inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 8)\n",
      "[0.        0.        0.        0.        0.        0.        0.\n",
      " 1.2673484 0.        0.       ]\n",
      "[0.         0.43734848 4.0474186  2.0499163  2.4249654  1.4971675\n",
      " 0.         0.         0.         0.8383116 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
    "    tblock = TemporalBlock(8, 2, 1, 1)\n",
    "    output = tblock(x, training=tf.constant(True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, 0])\n",
    "    print(res[1, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConvNet(tf.layers.Layer):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalConvNet, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
    "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        outputs = inputs\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs, training=training)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of levels 4\n",
      "downsampling...\n",
      "(32, 10, 8)\n",
      "[0.         1.1321595  0.         0.68209094 0.         0.7974938\n",
      " 0.         0.         2.4094856  2.0317686 ]\n",
      "[2.1969428  6.531462   6.1554384  1.487592   0.         0.26302177\n",
      " 0.93529415 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
    "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
    "    output = tcn(x, training=tf.constant(True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, 0])\n",
    "    print(res[1, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_classifier(x, y, dropout, opt, is_training):\n",
    "    num_blocks = 6\n",
    "    tblock = x\n",
    "    layers = []\n",
    "    for i in range(num_blocks):\n",
    "        opt.l = 2 ** i\n",
    "        tblock = temporal_block(tblock, dropout, opt, is_training)\n",
    "#     # first block\n",
    "#     opt.l = 1\n",
    "#     layer_1 = temporal_block(x, dropout, opt, is_training)\n",
    "#     # second block\n",
    "#     opt.l = 2\n",
    "#     layer_2 = temporal_block(layer_1, dropout, opt, is_training)\n",
    "#     # third block\n",
    "#     opt.l = 4\n",
    "#     layer_3 = temporal_block(layer_2, dropout, opt, is_training)\n",
    "#     # fourth block\n",
    "#     opt.l = 8\n",
    "#     layer_4 = temporal_block(layer_3, dropout, opt, is_training)\n",
    "#     # print(layer_3.shape)\n",
    "#     # print(layer_3[:, -1, :].shape)\n",
    "    H_enc_fin = tblock[:, -1, :]\n",
    "\n",
    "    logits = tf.layers.dense(H_enc_fin, 10, activation=None, kernel_initializer=tf.orthogonal_initializer())\n",
    "#     logits = tf.layers.dense(\n",
    "#         TemporalConvNet([20] * 6, opt.num_filters, 1-dropout)(\n",
    "#             X, training=is_training)[:, -1, :],\n",
    "#         opt.num_classes, activation=None, \n",
    "#         kernel_initializer=tf.orthogonal_initializer()\n",
    "#     )\n",
    "    prob = tf.nn.softmax(logits)\n",
    "#     print(H_enc_fin.shape, logits.shape, y.shape)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_step = tf.train.AdamOptimizer(opt.lr_rate).minimize(loss)\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "    return prob, loss, train_step, accuracy, saver, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches per epoch: 859\n"
     ]
    }
   ],
   "source": [
    "opt.l = 1\n",
    "opt.k = 20\n",
    "\n",
    "opt.num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "opt.timestep = 28 * 28 # timesteps\n",
    "opt.num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "opt.keep_prob = 0.9\n",
    "opt.num_filters = 20\n",
    "\n",
    "# Training Parameters\n",
    "opt.lr_rate = 0.001\n",
    "opt.batch_size = 64\n",
    "display_step = 500\n",
    "total_batch = int(mnist.train.num_examples / opt.batch_size)\n",
    "print(\"Number of batches per epoch:\", total_batch)\n",
    "training_steps = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_block(x, dropout, opt, is_training):\n",
    "    print(\"---- dialation {0} ----\".format(opt.l))\n",
    "    padding = (opt.k - 1) * opt.l\n",
    "    # masked note embeddings\n",
    "    # x_masked_notes = tf.multiply(x, x_mask_notes)\n",
    "    x_masked_notes = x\n",
    "    tcn_1 = CausalConv1D(opt.num_filters, opt.k, dilation_rate=opt.l, activation=None)(x)[0]\n",
    "#     print(tcn_1[0].shape, tcn_1[1].shape)\n",
    "#     x_padded = tf.pad(x_masked_notes, tf.constant([(0, 0), (padding, 0), (0, 0)]))\n",
    "    # 1st tcn layer with dialation rate l and kernel size k\n",
    "#     tcn_1 = tf.layers.conv1d(x_padded, filters=opt.num_filters, kernel_size=opt.k, padding='valid',\n",
    "#                              dilation_rate=opt.l, activation=None)\n",
    "    tcn_1_norm = tf.nn.relu(tf.contrib.layers.layer_norm(tcn_1))\n",
    "    tcn_1_output = tf.layers.dropout(tcn_1_norm, rate=dropout, training=is_training, \n",
    "                                     noise_shape = [tf.constant(1), tf.constant(1), tf.constant(opt.num_filters)])\n",
    "    # print(tcn_1_output.shape)\n",
    "    # 2nd tcn layer with same specs\n",
    "    # tcn_1_output_masked = tf.multiply(tcn_1_output, x_mask_notes)\n",
    "    tcn_1_output_masked = tcn_1_output\n",
    "#     x_padded_2 = tf.pad(tcn_1_output_masked, tf.constant([(0, 0), (padding, 0), (0, 0)]))\n",
    "#     tcn_2 = tf.layers.conv1d(x_padded_2, filters=opt.num_filters, kernel_size=opt.k, padding='valid',\n",
    "#                              dilation_rate=opt.l, activation=None)\n",
    "    tcn_2 = CausalConv1D(opt.num_filters, opt.k, dilation_rate=opt.l, activation=None)(tcn_1_output_masked)[0]\n",
    "    tcn_2_norm = tf.nn.relu(tf.contrib.layers.layer_norm(tcn_2))\n",
    "    tcn_2_output = tf.layers.dropout(tcn_2_norm, rate=dropout, training=is_training,\n",
    "                                    noise_shape = [tf.constant(1), tf.constant(1), tf.constant(opt.num_filters)])\n",
    "    print(tcn_2_output.shape)\n",
    "    return tcn_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- dialation 1 ----\n",
      "(?, 784, 20)\n",
      "---- dialation 2 ----\n",
      "(?, 784, 20)\n",
      "---- dialation 4 ----\n",
      "(?, 784, 20)\n",
      "---- dialation 8 ----\n",
      "(?, 784, 20)\n",
      "---- dialation 16 ----\n",
      "(?, 784, 20)\n",
      "---- dialation 32 ----\n",
      "(?, 784, 20)\n",
      "All parameters: 267992.0\n",
      "Trainable parameters: 89330\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, opt.timestep, opt.num_input], name='X')\n",
    "    Y = tf.placeholder(\"float\", [None, opt.num_classes], name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob_')\n",
    "    is_training = tf.placeholder(\"bool\", name=\"trainable_\")\n",
    "    prob_, loss_, train_step_, acc_, saver_, init = emb_classifier(X, Y, keep_prob, opt, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 3.9256, Training Accuracy= 0.109, Test Accuracy= 0.086\n",
      "Step 500, Minibatch Loss= 2.3747, Training Accuracy= 0.016, Test Accuracy= 0.078\n",
      "Step 1000, Minibatch Loss= 2.3196, Training Accuracy= 0.078, Test Accuracy= 0.078\n",
      "Step 1500, Minibatch Loss= 2.3290, Training Accuracy= 0.047, Test Accuracy= 0.078\n",
      "Step 2000, Minibatch Loss= 2.2950, Training Accuracy= 0.125, Test Accuracy= 0.078\n",
      "Step 2500, Minibatch Loss= 2.2945, Training Accuracy= 0.125, Test Accuracy= 0.094\n",
      "Step 3000, Minibatch Loss= 2.3012, Training Accuracy= 0.125, Test Accuracy= 0.094\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(opt.batch_size)\n",
    "#         print(np.max(batch_x), np.mean(batch_x), np.median(batch_x))\n",
    "        # Reshape data to get 28 * 28 seq of 1 elements\n",
    "        batch_x = batch_x.reshape((opt.batch_size, opt.timestep, opt.num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        loss, _ = sess.run([loss_, train_step_], feed_dict={X: batch_x, Y: batch_y, keep_prob: opt.keep_prob, is_training: True})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_, acc_], feed_dict={\n",
    "                X: batch_x, Y: batch_y, keep_prob:1.0, is_training: False})\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, opt.timestep, opt.num_input))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            val_acc = sess.run(acc_, feed_dict={X: test_data, Y: test_label, keep_prob:1.0,is_training: False})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver_.save(sess, \"./save/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
