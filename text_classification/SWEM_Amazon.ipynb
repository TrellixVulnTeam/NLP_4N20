{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEM On Amazon Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from keras import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dat_path = \"./data/\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from utils import tokenizer, word2id_id2word_dicts, prepare_data_for_emb, token_to_id,\\\n",
    "    load_embedding_vectors_glove_gensim, get_minibatches_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model functions\n",
    "from model import embedding, aver_emb_encoder, max_emb_encoder, concat_emb_encoder,\\\n",
    "    discriminator_2layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embd = dat_path + 'embeddings/glove.6B.300d.txt.word2vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Amazon Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(dat_path + \"amazonreviews/train.ft.txt\", sep = \"\\t\", names = [\"row\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform input data to a workable dataframe\n",
    "# Input: text data\n",
    "# Output: pandas dataframe\n",
    "def dat_prepare(df):\n",
    "    new = df\n",
    "    new = pd.DataFrame(new.row.str.split(':', 1).tolist(), columns = ['label','review'])\n",
    "    new1 = new\n",
    "    new1 = pd.DataFrame(new1.label.str.split(' ', 1).tolist(), columns = ['label','content'])\n",
    "    new1['review'] = new['review']\n",
    "    return new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dat_prepare(df_raw)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset 10000 samples from the entire dataset\n",
    "# Subset train/test data from the original dataset to reduce the computing cost\n",
    "n = 10000\n",
    "review_data = df[0:n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data\n",
    "x_data = review_data['review']\n",
    "\n",
    "y_label = review_data['label']\n",
    "y_label = np.where(y_label == '__label__2', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt(object):\n",
    "    def __init__(self):\n",
    "        # Maximum number of words in a review\n",
    "        self.embeddings = None\n",
    "        self.maxlen = None\n",
    "        # Number of unique words in all reviews\n",
    "        self.uniq_words = None\n",
    "        # Vector size for each word embeddings from GloVe\n",
    "        self.emb_size = 300\n",
    "        # Training Batch Size\n",
    "        self.batch_size = 50\n",
    "        # Epoch\n",
    "        self.epoch = 101\n",
    "        # Learning rate\n",
    "        self.lr_rate = 0.01\n",
    "        # Dropout - keep rate\n",
    "        self.dropout = 0.5\n",
    "        # Number of hidden layers\n",
    "        self.H_dis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model options\n",
    "opt = opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish tokenizing...\n",
      "Finish creating word2id, id2word matrices 4471 4471\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "x_tokenized = x_data.apply(tokenizer)\n",
    "print(\"Finish tokenizing...\")\n",
    "\n",
    "# word2id, id2word\n",
    "x_word2id = word2id_id2word_dicts(x_tokenized)[0]\n",
    "x_id2word = word2id_id2word_dicts(x_tokenized)[1]\n",
    "print(\"Finish creating word2id, id2word matrices\", len(x_word2id), len(x_id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish creating token to id matrix:  10000\n"
     ]
    }
   ],
   "source": [
    "# assigning ids to tokens\n",
    "token_id_mat = [token_to_id(x, x_word2id) for x in x_tokenized]\n",
    "print(\"Finish creating token to id matrix: \", len(token_id_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-19 23:59:08,495 : INFO : loading projection weights from ~/Documents/DCRI/PAD_ML/Code/data/embeddings/glove.6B.300d.txt.word2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-20 00:00:46,494 : INFO : loaded (400000, 300) matrix from ~/Documents/DCRI/PAD_ML/Code/data/embeddings/glove.6B.300d.txt.word2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocab in glove: 4402\n",
      "num of vocab not in glove: 69\n"
     ]
    }
   ],
   "source": [
    "# Word Vectors\n",
    "x_word_embeddings = load_embedding_vectors_glove_gensim(x_word2id, glove_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.maxlen = np.max([len(s) for s in x_tokenized])\n",
    "opt.uniq_words = len(x_word2id)\n",
    "opt.embeddings = x_word_embeddings\n",
    "opt.H_dis = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "n_train = int(n*spl_ratio)\n",
    "n_val = int(n*val_ratio)\n",
    "n_test = int(n*test_ratio)\n",
    "train = token_id_mat[0:n_train]\n",
    "val = token_id_mat[n_train:n_train+n_val]\n",
    "test = token_id_mat[n_train+n_val:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = y_label[0:n_train]\n",
    "val_lab = y_label[n_train:n_train+n_val]\n",
    "test_lab = y_label[n_train+n_val:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SWEM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_classifier_max(x, x_mask, y, dropout, opt):\n",
    "    # print x.get_shape()  # batch L\n",
    "    x_emb, W_emb = embedding(x, opt)  # batch L emb\n",
    "    print(\"size of embedding: \", x_emb.shape)\n",
    "    x_emb = tf.expand_dims(x_emb, 3)  # batch L emb 1\n",
    "    \n",
    "    print(\"Input size for encoding x_emb: \", x_emb.shape)\n",
    "    H_enc = max_emb_encoder(x_emb, x_mask, opt)\n",
    "    \n",
    "    print(\"Encoder shape: \", H_enc.shape)\n",
    "    \n",
    "    logits = discriminator_2layer(H_enc, opt, dropout, prefix='classify_', num_outputs=1, is_reuse=None)\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prob), y), tf.float32))\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # Training Step\n",
    "    train_step = tf.train.AdamOptimizer(opt.lr_rate).minimize(loss)   \n",
    "    return prob, accuracy, loss, train_step, W_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_classifier_aver(x, x_mask, y, dropout, opt):\n",
    "    # print x.get_shape()  # batch L\n",
    "    x_emb, W_emb = embedding(x, opt)  # batch L emb\n",
    "    print(\"size of embedding: \", x_emb.shape)\n",
    "    x_emb = tf.expand_dims(x_emb, 3)  # batch L emb 1\n",
    "    \n",
    "    print(\"Input size for encoding x_emb: \", x_emb.shape)\n",
    "    H_enc = aver_emb_encoder(x_emb, x_mask)\n",
    "    \n",
    "    print(\"Encoder shape: \", H_enc.shape)\n",
    "    \n",
    "    logits = discriminator_2layer(H_enc, opt, dropout, prefix='classify_', num_outputs=1, is_reuse=None)\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prob), y), tf.float32))\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # Training Step\n",
    "    train_step = tf.train.AdamOptimizer(opt.lr_rate).minimize(loss)   \n",
    "    return prob, accuracy, loss, train_step, W_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_classifier_concat(x, x_mask, y, dropout, opt):\n",
    "    # print x.get_shape()  # batch L\n",
    "    x_emb, W_emb = embedding(x, opt)  # batch L emb\n",
    "    print(\"size of embedding: \", x_emb.shape)\n",
    "    x_emb = tf.expand_dims(x_emb, 3)  # batch L emb 1\n",
    "    \n",
    "    print(\"Input size for encoding x_emb: \", x_emb.shape)\n",
    "    H_enc = concat_emb_encoder(x_emb, x_mask, opt)\n",
    "    \n",
    "    print(\"Encoder shape: \", H_enc.shape)\n",
    "    \n",
    "    logits = discriminator_2layer(H_enc, opt, dropout, prefix='classify_', num_outputs=1, is_reuse=None)\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prob), y), tf.float32))\n",
    "\n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # Training Step\n",
    "    train_step = tf.train.AdamOptimizer(opt.lr_rate).minimize(loss)   \n",
    "    return prob, accuracy, loss, train_step, W_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Model (Max pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize word embedding finished\n",
      "size of embedding:  (?, 206, 300)\n",
      "Input size for encoding x_emb:  (?, 206, 300, 1)\n",
      "Encoder shape:  (?, 600)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Define the model input\n",
    "x_ = tf.placeholder(tf.int32, shape=[None, opt.maxlen])\n",
    "x_mask_ = tf.placeholder(tf.float32, shape=[None, opt.maxlen])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "prob_, accuracy_, loss_, train_step, W_emb_ = emb_classifier_concat(x_, x_mask_, y_, keep_prob, opt)\n",
    "# prob_, accuracy_, loss_, train_step, W_emb_ = emb_classifier_max(x_, x_mask_, y_, keep_prob, opt)\n",
    "# prob_, accuracy_, loss_, train_step, W_emb_ = emb_classifier_aver(x_, x_mask_, y_, keep_prob, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.818000 \n",
      "Validation accuracy 0.836000 \n",
      "Epoch: 0 \t Train Loss: 0.5054317116737366\n",
      "------------------------------------------------------\n",
      "Train accuracy 0.836000 \n",
      "Validation accuracy 0.853000 \n",
      "Epoch: 1 \t Train Loss: 0.49620312452316284\n",
      "------------------------------------------------------\n",
      "Train accuracy 0.853000 \n",
      "Validation accuracy 0.837000 \n",
      "Epoch: 2 \t Train Loss: 0.4026777744293213\n",
      "------------------------------------------------------\n",
      "Train accuracy 0.837000 \n",
      "Validation accuracy 0.843000 \n",
      "Epoch: 3 \t Train Loss: 0.20640811324119568\n",
      "------------------------------------------------------\n",
      "Train accuracy 0.843000 \n",
      "Validation accuracy 0.845000 \n",
      "Epoch: 4 \t Train Loss: 0.3084617555141449\n",
      "------------------------------------------------------\n",
      "Train accuracy 0.845000 \n",
      "Validation accuracy 0.836000 \n",
      "Early Stopping. \n",
      "Epoch: 5 \t Val Acc: 0.835999995470047 \t Train Acc: 0.9166249975562095\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.8409999966621399 Test AUC 0.8652597402597402\n"
     ]
    }
   ],
   "source": [
    "opt.epoch = 10\n",
    "max_val_acc = 0\n",
    "counter = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(opt.epoch):\n",
    "        train_correct = 0.0\n",
    "        kf = get_minibatches_idx(n_train, opt.batch_size, shuffle=True)\n",
    "        for _, train_index in kf:\n",
    "            sents = [train[t] for t in train_index]\n",
    "            x_labels = [train_lab[t] for t in train_index]\n",
    "            x_labels = np.array(x_labels)\n",
    "            x_labels = x_labels.reshape([-1,1])\n",
    "            \n",
    "            x_batch, x_batch_mask = prepare_data_for_emb(sents, opt)\n",
    "            train_acc, _, loss = sess.run([accuracy_, train_step, loss_], feed_dict={x_: x_batch, x_mask_: x_batch_mask, y_: x_labels, keep_prob: opt.dropout})\n",
    "            \n",
    "            train_correct += train_acc * len(train_index)\n",
    "        train_accuracy = train_correct / n_train\n",
    "        print(\"Train accuracy %f \" % val_accuracy)\n",
    "        \n",
    "        # Early Stopping Validation\n",
    "        val_correct = 0.0\n",
    "        kf_val = get_minibatches_idx(n_val, opt.batch_size, shuffle=True)\n",
    "        for _, val_index in kf_val:\n",
    "            val_sents = [val[t] for t in val_index]\n",
    "            val_labels = [val_lab[t] for t in val_index]\n",
    "            val_labels = np.array(val_labels)\n",
    "            val_labels = val_labels.reshape([-1,1])\n",
    "            x_val_batch, x_val_batch_mask = prepare_data_for_emb(val_sents, opt)\n",
    "\n",
    "            val_prob, val_acc = sess.run([prob_, accuracy_], feed_dict={x_: x_val_batch, x_mask_: x_val_batch_mask,\n",
    "                                                          y_: val_labels, keep_prob: 1.0})\n",
    "\n",
    "            val_true_lab = val_labels\n",
    "            val_correct += val_acc * len(val_index)\n",
    "\n",
    "        val_accuracy = val_correct / n_val\n",
    "        print(\"Validation accuracy %f \" % val_accuracy)\n",
    "\n",
    "        if val_accuracy > max_val_acc:\n",
    "            max_val_acc = val_accuracy\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter > 3:\n",
    "            print(\"Early Stopping. \")\n",
    "            print(\"Epoch: {0} \\t Val Acc: {1} \\t Train Acc: {2}\".format(epoch, val_accuracy, train_accuracy))\n",
    "            test_correct = 0.0\n",
    "            kf_test = get_minibatches_idx(n_test, opt.batch_size, shuffle=True)\n",
    "            for _, test_index in kf_test:\n",
    "                test_sents = [test[t] for t in test_index]\n",
    "                test_labels = [test_lab[t] for t in test_index]\n",
    "                test_labels = np.array(test_labels)\n",
    "                test_labels = test_labels.reshape([-1,1])\n",
    "                x_test_batch, x_test_batch_mask = prepare_data_for_emb(test_sents, opt)\n",
    "\n",
    "                test_prob, test_acc = sess.run([prob_, accuracy_],\n",
    "                                         feed_dict={x_: x_test_batch, x_mask_: x_test_batch_mask,\n",
    "                                                    y_: test_labels, keep_prob: 1.0})\n",
    "\n",
    "                test_correct += test_acc * len(test_index)\n",
    "\n",
    "            test_accuracy = test_correct / len(test)\n",
    "            \n",
    "            fpr, tpr, threshold = metrics.roc_curve(test_labels, test_prob)\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            plt.figure()\n",
    "            plt.title('Receiver Operating Characteristic')\n",
    "            plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "            plt.legend(loc = 'lower right')\n",
    "            plt.plot([0, 1], [0, 1],'r--')\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Test accuracy {0} Test AUC {1}\".format(test_accuracy, roc_auc))\n",
    "            break\n",
    "\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1}\".format(epoch, loss))\n",
    "        print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Starting epoch 0\n",
      "-------------------------------------------------------\n",
      "Iteration 20: Training loss 0.692127 \n",
      "Train accuracy 0.526000 \n",
      "Validation accuracy 0.541000 \n",
      "Test accuracy 0.537000 \n",
      "-------------------------------------------------------\n",
      "Iteration 40: Training loss 0.693328 \n",
      "Train accuracy 0.506000 \n",
      "Validation accuracy 0.481000 \n",
      "-------------------------------------------------------\n",
      "Iteration 60: Training loss 0.692074 \n",
      "Train accuracy 0.494000 \n",
      "Validation accuracy 0.519000 \n",
      "-------------------------------------------------------\n",
      "Iteration 80: Training loss 0.686325 \n",
      "Train accuracy 0.494000 \n",
      "Validation accuracy 0.519000 \n",
      "-------------------------------------------------------\n",
      "Iteration 100: Training loss 0.691715 \n",
      "Train accuracy 0.494000 \n",
      "Validation accuracy 0.519000 \n",
      "-------------------------------------------------------\n",
      "Iteration 120: Training loss 0.694553 \n",
      "Train accuracy 0.494000 \n",
      "Validation accuracy 0.519000 \n",
      "-------------------------------------------------------\n",
      "Iteration 140: Training loss 0.664190 \n",
      "Train accuracy 0.710000 \n",
      "Validation accuracy 0.686000 \n",
      "Test accuracy 0.656000 \n",
      "-------------------------------------------------------\n",
      "Iteration 160: Training loss 0.652894 \n",
      "Train accuracy 0.774000 \n",
      "Validation accuracy 0.758000 \n",
      "Test accuracy 0.705000 \n",
      "Epoch 0: Max Test accuracy 0.705000\n",
      "====================================================\n",
      "Starting epoch 1\n",
      "-------------------------------------------------------\n",
      "Iteration 180: Training loss 0.619302 \n",
      "Train accuracy 0.798000 \n",
      "Validation accuracy 0.780000 \n",
      "Test accuracy 0.787000 \n",
      "-------------------------------------------------------\n",
      "Iteration 200: Training loss 0.450566 \n",
      "Train accuracy 0.822000 \n",
      "Validation accuracy 0.789000 \n",
      "Test accuracy 0.793000 \n",
      "-------------------------------------------------------\n",
      "Iteration 220: Training loss 0.538018 \n",
      "Train accuracy 0.828000 \n",
      "Validation accuracy 0.792000 \n",
      "Test accuracy 0.791000 \n",
      "-------------------------------------------------------\n",
      "Iteration 240: Training loss 0.625389 \n",
      "Train accuracy 0.820000 \n",
      "Validation accuracy 0.762000 \n",
      "-------------------------------------------------------\n",
      "Iteration 260: Training loss 0.390660 \n",
      "Train accuracy 0.822000 \n",
      "Validation accuracy 0.796000 \n",
      "Test accuracy 0.737000 \n",
      "-------------------------------------------------------\n",
      "Iteration 280: Training loss 0.633570 \n",
      "Train accuracy 0.870000 \n",
      "Validation accuracy 0.838000 \n",
      "Test accuracy 0.812000 \n",
      "-------------------------------------------------------\n",
      "Iteration 300: Training loss 0.607884 \n",
      "Train accuracy 0.880000 \n",
      "Validation accuracy 0.840000 \n",
      "Test accuracy 0.818000 \n",
      "-------------------------------------------------------\n",
      "Iteration 320: Training loss 0.597408 \n",
      "Train accuracy 0.878000 \n",
      "Validation accuracy 0.848000 \n",
      "Test accuracy 0.804000 \n",
      "Epoch 1: Max Test accuracy 0.804000\n",
      "====================================================\n",
      "Starting epoch 2\n",
      "-------------------------------------------------------\n",
      "Iteration 340: Training loss 0.456798 \n",
      "Train accuracy 0.898000 \n",
      "Validation accuracy 0.842000 \n",
      "-------------------------------------------------------\n",
      "Iteration 360: Training loss 0.489391 \n",
      "Train accuracy 0.888000 \n",
      "Validation accuracy 0.818000 \n",
      "-------------------------------------------------------\n",
      "Iteration 380: Training loss 0.312022 \n",
      "Train accuracy 0.900000 \n",
      "Validation accuracy 0.848000 \n",
      "-------------------------------------------------------\n",
      "Iteration 400: Training loss 0.458239 \n",
      "Train accuracy 0.914000 \n",
      "Validation accuracy 0.837000 \n",
      "-------------------------------------------------------\n",
      "Iteration 420: Training loss 0.468499 \n",
      "Train accuracy 0.910000 \n",
      "Validation accuracy 0.847000 \n",
      "-------------------------------------------------------\n",
      "Iteration 440: Training loss 0.703676 \n",
      "Train accuracy 0.908000 \n",
      "Validation accuracy 0.860000 \n",
      "Test accuracy 0.820000 \n",
      "-------------------------------------------------------\n",
      "Iteration 460: Training loss 0.416330 \n",
      "Train accuracy 0.906000 \n",
      "Validation accuracy 0.860000 \n",
      "-------------------------------------------------------\n",
      "Iteration 480: Training loss 0.297623 \n",
      "Train accuracy 0.920000 \n",
      "Validation accuracy 0.846000 \n",
      "Epoch 2: Max Test accuracy 0.820000\n",
      "====================================================\n",
      "Starting epoch 3\n",
      "-------------------------------------------------------\n",
      "Iteration 500: Training loss 0.377221 \n",
      "Train accuracy 0.934000 \n",
      "Validation accuracy 0.841000 \n",
      "-------------------------------------------------------\n",
      "Iteration 520: Training loss 0.573369 \n",
      "Train accuracy 0.918000 \n",
      "Validation accuracy 0.818000 \n",
      "-------------------------------------------------------\n",
      "Iteration 540: Training loss 0.396029 \n",
      "Train accuracy 0.938000 \n",
      "Validation accuracy 0.839000 \n",
      "-------------------------------------------------------\n",
      "Iteration 560: Training loss 0.438381 \n",
      "Train accuracy 0.938000 \n",
      "Validation accuracy 0.841000 \n",
      "-------------------------------------------------------\n",
      "Iteration 580: Training loss 0.407032 \n",
      "Train accuracy 0.928000 \n",
      "Validation accuracy 0.835000 \n",
      "-------------------------------------------------------\n",
      "Iteration 600: Training loss 0.339514 \n",
      "Train accuracy 0.934000 \n",
      "Validation accuracy 0.840000 \n",
      "-------------------------------------------------------\n",
      "Iteration 620: Training loss 0.282644 \n",
      "Train accuracy 0.942000 \n",
      "Validation accuracy 0.841000 \n",
      "-------------------------------------------------------\n",
      "Iteration 640: Training loss 0.420822 \n",
      "Train accuracy 0.940000 \n",
      "Validation accuracy 0.846000 \n",
      "Epoch 3: Max Test accuracy 0.820000\n",
      "====================================================\n",
      "Starting epoch 4\n",
      "-------------------------------------------------------\n",
      "Iteration 660: Training loss 0.438904 \n",
      "Train accuracy 0.948000 \n",
      "Validation accuracy 0.853000 \n",
      "-------------------------------------------------------\n",
      "Iteration 680: Training loss 0.449120 \n",
      "Train accuracy 0.944000 \n",
      "Validation accuracy 0.847000 \n",
      "-------------------------------------------------------\n",
      "Iteration 700: Training loss 0.395114 \n",
      "Train accuracy 0.906000 \n",
      "Validation accuracy 0.800000 \n",
      "-------------------------------------------------------\n",
      "Iteration 720: Training loss 0.434123 \n",
      "Train accuracy 0.944000 \n",
      "Validation accuracy 0.842000 \n",
      "-------------------------------------------------------\n",
      "Iteration 740: Training loss 0.327442 \n",
      "Train accuracy 0.938000 \n",
      "Validation accuracy 0.837000 \n",
      "-------------------------------------------------------\n",
      "Iteration 760: Training loss 0.406206 \n",
      "Train accuracy 0.942000 \n",
      "Validation accuracy 0.832000 \n",
      "-------------------------------------------------------\n",
      "Iteration 780: Training loss 0.526847 \n",
      "Train accuracy 0.944000 \n",
      "Validation accuracy 0.839000 \n",
      "-------------------------------------------------------\n",
      "Iteration 800: Training loss 0.575385 \n",
      "Train accuracy 0.950000 \n",
      "Validation accuracy 0.847000 \n",
      "Epoch 4: Max Test accuracy 0.820000\n",
      "Max Test accuracy 0.820000 \n"
     ]
    }
   ],
   "source": [
    "uidx = 0\n",
    "max_val_accuracy = 0.\n",
    "max_test_accuracy = 0.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        for epoch in range(5):\n",
    "            print(\"====================================================\")\n",
    "            print(\"Starting epoch %d\" % epoch)\n",
    "            kf = get_minibatches_idx(n_train, opt.batch_size, shuffle=True)\n",
    "            for _, train_index in kf:\n",
    "                uidx += 1\n",
    "                sents = [train[t] for t in train_index]\n",
    "                x_labels = [train_lab[t] for t in train_index]\n",
    "                x_labels = np.array(x_labels)\n",
    "                x_labels = x_labels.reshape([-1,1])\n",
    "\n",
    "                x_batch, x_batch_mask = prepare_data_for_emb(sents, opt)\n",
    "                _, loss = sess.run([train_step, loss_], feed_dict={x_: x_batch, x_mask_: x_batch_mask, y_: x_labels, keep_prob: opt.dropout})\n",
    "                \n",
    "                # Validation for every 20 iterations\n",
    "                if uidx % 20 == 0:\n",
    "                    train_correct = 0.0\n",
    "                    kf_train = get_minibatches_idx(500, opt.batch_size, shuffle=True)\n",
    "                    for _, train_index in kf_train:\n",
    "                        train_sents = [train[t] for t in train_index]\n",
    "                        train_labels = [train_lab[t] for t in train_index]\n",
    "                        train_labels = np.array(train_labels)\n",
    "                        train_labels = train_labels.reshape([-1,1])\n",
    "                        x_train_batch, x_train_batch_mask = prepare_data_for_emb(train_sents, opt)  # Batch L\n",
    "\n",
    "                        train_accuracy = sess.run(accuracy_, feed_dict={x_: x_train_batch, x_mask_: x_train_batch_mask, y_: train_labels, keep_prob: 1.0})\n",
    "\n",
    "                        train_correct += train_accuracy * len(train_index)\n",
    "\n",
    "                    train_accuracy = train_correct / 500\n",
    "                    print(\"-------------------------------------------------------\")\n",
    "                    print(\"Iteration %d: Training loss %f \" % (uidx, loss))\n",
    "                    print(\"Train accuracy %f \" % train_accuracy)\n",
    "\n",
    "                    val_correct = 0.0\n",
    "                    kf_val = get_minibatches_idx(n_val, opt.batch_size, shuffle=True)\n",
    "                    for _, val_index in kf_val:\n",
    "                        val_sents = [val[t] for t in val_index]\n",
    "                        val_labels = [val_lab[t] for t in val_index]\n",
    "                        val_labels = np.array(val_labels)\n",
    "                        val_labels = val_labels.reshape([-1,1])\n",
    "                        x_val_batch, x_val_batch_mask = prepare_data_for_emb(val_sents, opt)\n",
    "\n",
    "                        val_accuracy = sess.run(accuracy_, feed_dict={x_: x_val_batch, x_mask_: x_val_batch_mask,\n",
    "                                                                      y_: val_labels, keep_prob: 1.0})\n",
    "\n",
    "                        val_correct += val_accuracy * len(val_index)\n",
    "\n",
    "                    val_accuracy = val_correct / n_val\n",
    "                    print(\"Validation accuracy %f \" % val_accuracy)\n",
    "\n",
    "                    if val_accuracy > max_val_accuracy:\n",
    "                        max_val_accuracy = val_accuracy\n",
    "\n",
    "                        test_correct = 0.0\n",
    "                        kf_test = get_minibatches_idx(n_test, opt.batch_size, shuffle=True)\n",
    "                        for _, test_index in kf_test:\n",
    "                            test_sents = [test[t] for t in test_index]\n",
    "                            test_labels = [test_lab[t] for t in test_index]\n",
    "                            test_labels = np.array(test_labels)\n",
    "                            test_labels = test_labels.reshape([-1,1])\n",
    "                            x_test_batch, x_test_batch_mask = prepare_data_for_emb(test_sents, opt)\n",
    "\n",
    "                            test_accuracy = sess.run(accuracy_,\n",
    "                                                     feed_dict={x_: x_test_batch, x_mask_: x_test_batch_mask,\n",
    "                                                                y_: test_labels, keep_prob: 1.0})\n",
    "\n",
    "                            test_correct += test_accuracy * len(test_index)\n",
    "\n",
    "                        test_accuracy = test_correct / n_test\n",
    "\n",
    "                        print(\"Test accuracy %f \" % test_accuracy)\n",
    "\n",
    "                        max_test_accuracy = test_accuracy\n",
    "\n",
    "            print(\"Epoch %d: Max Test accuracy %f\" % (epoch, max_test_accuracy))\n",
    "\n",
    "#             emb = sess.run(W_emb_, feed_dict={x_: x_test_batch})\n",
    "\n",
    "#             cPickle.dump([emb], open(\"yahoo_emb_max_300.p\", \"wb\"))\n",
    "\n",
    "        print(\"Max Test accuracy %f \" % max_test_accuracy)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # print 'Training interupted'\n",
    "        print('Training interupted')\n",
    "        print(\"Max Test accuracy %f \" % max_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
